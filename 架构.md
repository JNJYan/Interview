<!--

 * @Author: JNJYan
 * @LastEditors: JNJYan
 * @Email: jjy20140825@gmail.com
 * @Date: 2020-07-09 14:13:08
 * @LastEditTime: 2020-07-09 20:56:11
 * @Description: Modify here please
 * @FilePath: /Interview/架构.md
--> 
[TOC]
# 架构的目的
主要目的是为了解决软件复杂度带来的问题，复杂度的来源有：高性能、高可用、可拓展、低成本、安全、规模。

## 高性能

## 高可用
计算高可用、存储高可用

状态决策：独裁式、协商式、民主式

ZooKeeper选举算法Paxos

民主式决策固有缺陷：脑裂现象，原来统一的集群因为连接中断，造成两个独立分割的子集群。

## 可拓展

## 低成本
1. NoSQl(Memcache, Redis)的出现是为了解决关系型数据库无法应对高并发访问带来的访问压力。
2. 全文搜索引擎(Elasticsearch)的出现是为了解决关系型数据库like搜索的低效问题。
3. Hadoop的出现是为了解决传统文件系统无法应对海量数据存储和计算的问题。

## 安全
1. 功能安全
2. 架构安全

## 规模
量变引起质变，功能增多，系统复杂度指数级上升。数据增多，系统复杂度发生质变。

# 架构设计三原则
## 合适原则
“合适优于业界领先”

## 简单原则
简单优于复杂

复杂性体现在两个方面

**结构复杂性**：复杂系统的组件数量更多，组件之间的关系更加复杂。

**逻辑复杂性**：单个组件承担太多功能。

## 演化原则
演化优于一步到位

## 案例
### 阿里
1. 买个轻量级网站，二次开发
主从MySQL

2. Oracle/支付宝
Oracle+NAS(Network Attached Storage，网络附属存储)+Oracle RAC(Real Application Clusters，实时应用集群实现负载均衡)。

3. PHP->JAVA1.0

4. Java2.0
分库、引入Spring、缓存、CDN、开源JBoss

5. Java3.0/分布式
自研

### 手机QQ
用户规模
1. 十万级
存储服务器<-->接入服务器

2. 百万级
存储集群<-->接入集群
状态同步服务器、长连接集群

3. 千万级
千万级时产生问题：
   - 同步流量太大，状态同步服务器单机瓶颈
   - 在线用户信息量太大，单台接入服务器村放不下
   - 单台状态同步服务器放不下所有在线用户
   - 单台接入服务器支撑不住所有在线用户的在线状态信息。

4. 亿级
    重新设计

# 架构设计流程
## 识别复杂度
在实际中往往不需要同时满足三个条件(高性能、高可用、可扩展)

## 设计备选方案
1. 备选方案3~5个最佳
2. 备选方案差异明显
3. 不局限于已熟悉的技术
4. 关注技术选型，而非技术细节

### 前浪微博案例
#### 案例详情
    用户发一条微博后，微博子系统需要通知审核子系统进行审核，然后通知统计子系统进行统计，再通知广告子系统进行广告预测，接着通知消息子系统进行消息推送……一条微博有十几个通知，目前都是系统间通过接口调用的。每通知一个新系统，微博子系统就要设计接口、进行测试，效率很低，问题定位很麻烦，经常和其他子系统的技术人员产生分岐，微博子系统的开发人员不胜其烦。
    用户等级达到 VIP 后，等级子系统要通知福利子系统进行奖品发放，要通知客服子系统安排专属服务人员，要通知商品子系统进行商品打折处理……等级子系统的开发人员也是不胜其烦。

#### 解决思想
根源在于架构上各业务子系统强耦合，而消息队列系统可以完成子系统的解耦，提议引入消息队列。

#### 备选方案
1. Kafka

成熟的开源消息队列方案，功能强大，性能高。

2. 集群+MySQL存储

    - 选用Netty高性能网络库。

    - 集群负载均衡算法采用简单轮询。

    - 为满足高可用读取，MySQL主备复制来实现高可用存储

3. 集群+自研存储方案

MySQL关系型数据库并不是很契合消息队列的数据特点，参考Kafka，自己实现一套文件存储和复制方案。

## 评估和选择备选方案

## 详细方案设计
Nginx的负载均衡策略
1. 轮询

2. 加权轮询
服务器性能不均。

3. ip_hash
每个访客固定访问一个后端服务器，用于解决session问题。

4. fair
根据服务器响应时间来分配请求，适用于后端服务器性能不均衡。

5. url_hash
按url的hash结果分配请求，适用于后端服务器能够将ur的响应结果缓存的情况。

# 高性能架构模式
## 高性能数据库集群
### 读写分离
本质是将访问压力分散到集群中的多个节点。

![image-20200709151714070](C:\Users\May\AppData\Roaming\Typora\typora-user-images\image-20200709151714070.png)

1. 数据库服务器搭建主从集群，一主一从或一主多从。
2. 主机负责写，从机负责读。
3. 主机通过复制将数据同步到从机。
4. 业务服务器将写操作发送给主机，读操作发送给从机。

#### 主从复制延迟
以MySQL为例，主从复制延迟可能达到1秒，如果有大量数据同步，延迟可能更长。

    用户注册完后立即登录，业务服务器提示还没有注册。

常用的几种解决方法：
1. 写操作后的读操作指定发送给数据库主服务器
对业务影响较大，易引入bug。

2. 读从机失败后读一次主机
实现代价较小，但将增加主机读操作压力，如遭遇黑客暴力破解账号，导致大量二次读取操作。

3. 关键业务读写操作指向主机，非关键业务读写分离
注册业务和修改个性签名任务(错了也没太大关系)。

#### 分配机制
将读写操作区分，访问不同的数据库服务器：程序代码封装和中间件封装。

程序代码封装：
1. 实现简单，可以根据业务做较多的定制化功能。
2. 每种语言都要重新写，重复开发工作量大。
3. 故障情况下，若发生主从切换，可能所有系统要更改配置重启。

中间件封装：
1. 支持多种编程语言，数据库中间件对业务服务器提供标准SQL接口。
2. 支持完整的SQL语法和数据库服务器的协议，实现较复杂，容易出错。
3. 中间件不执行读写，但所有操作请求都经过中间件，性能要求高。
4. 数据库主从切换对业务无感知，由中间件探测。

开源数据库中间件中，MySQL Proxy、 MySQL Router， Atlas(基于MySQL Proxy，360)
### 分库分表
分散访问压力和存储压力。

读写分离没有分散存储压力：
1. 数据量太大，读写性能下降，索引变得非常大，性能下降。
2. 数据文件很大，数据库备份和恢复需要耗费很长时间。
3. 数据文件越大，极端情况下丢失数据的风险越高。

#### 业务分库
按照业务模块将数据分散到不同的数据库服务器。但引入了新的问题。
1. 无法使用join操作，原本一次join查询能完成的操作，需要分别查询两个数据库。
2. 无法通过事务实现统一修改，性能太低。
3. 成本问题，还要做备份。

#### 分表
业务持续发展，同一业务的单表数据回答道单台数据库服务器的处理瓶颈。

单表数据拆分：
1. 垂直分表
原本一次查询可以获取所有字段，现在需要两次查询。

2. 水平分表
复杂表达到千万就要分表，简单表可能过亿也不需要。
    1. 路由选择难点
        - 范围路由: (选取有序字段时间戳、ID大小等)，易扩展表，但会造成分布不均。
        - Hash路由: 初始表数量选取难，一但确定，扩展麻烦， 但分布均匀。
        - 配置路由: 用一张独立的表记录路由信息，缺点在于需要多查一次配置路由表，影响性能。
    2. join操作
        数据分散于多个表，若要进行join查询，需要多次join查询，然后合并。
    3. count操作
        每个表单独进行，然后相加。可以增加一个记录数表，但同样会引入复杂度，且要保证同步操作，避免不一致。
    4. order by操作
        有业务代码或中间件分别查询然后汇总排序。

## 高性能NoSQL
### K-V存储
解决关系数据库无法存储数据结构的问题，value可以是各种数据结构，缺点不支持完整的ACID，常用的由Redis，memecachedb。

Redis缺点在于不支持完整ACID事务，虽然也提供事务，但只能保证隔离性和一致性，无法保证原子性和持久性。

### 文档数据库
解决关系数据库的强schema约束问题，存储的是json，数据字段的灵活性，不支持事务，无法实现join操作，MongoDB。

优势，新增字段简单，历史数据不会出错，容易存储复杂数据，适合电商筛选列表。

不支持事务，若使用MongoDB来存储商品库存，创建订单时要先扣减库存再创建订单，异常情况下可能扣减库存，但订单没有创建。

无法实现关系数据库的join操作，需要查多次。

### 列式存储数据库
解决关系数据库大数据场景下的I/O问题，按照列存储数据，HBase。

### 全文搜索引擎
解决关系数据的全文搜索性能问题，倒排索引，Elasticsearch。

## 高性能缓存架构
某些复杂业务场景下，单依靠存储系统的性能提升是不够的，典型场景有：
1. 需要复杂运算
实时显示论坛在线用户数，每次都要执行`count(*)`。

2. 读多写少的而数据
一人发微博，千万人来看。

### 缓存
基本原理是将可能重复使用的数据放到内存中，避免每次都访问存储系统。Memcache、Redis等。

单台Memcache服务器的简单key-value查询能够达到TPS50000以上。

### 缓存穿透
缓存没有发挥作用，先查缓存再查存储系统。

1. 存储数据不存在
    数据库中没有该数据，先查缓存再查存储系统，返回空。如黑客攻击。

    解决办法: 若存储系统中不存在，则设置默认值到缓存中。

2. 缓存数据生成耗费时间和资源
    爬虫等导致缓存不符合真实使用场景。
    解决办法: 检测爬虫行为，封禁。

### 缓存雪崩
缓存失效后引起的性能急剧下降，高并发场景下，多个线程都执行生成缓存。

1. 更新锁机制
对缓存更新加锁保护，保证只有一个线程能进行缓存更新，未获得更新锁的线程等待锁释放后读取或返回空值。对于分布式集群的业务系统，即使单台服务器只有一个线程更新缓存，仍然会造成雪崩，因此要采用分布式锁，如ZooKeeper。

2. 后台更新机制
    - 由后台线程更新缓存，而非业务线程，有效期设置位永久，后台线程定时更新。
    - 业务线程发现缓存失效后，通过消息队列，通知后台线程更新缓存。
    - 适用于单机多线程，也适用于分布式集群，比更新锁要简单，还适合缓存预热。

### 缓存热点
热点数据对缓存同样造成压力。

解决方案，复制多份缓存，将请求分散到多个缓存服务器上。不要设置统一的过期时间，否则会出现所有缓存副本同时失效的情况，引发雪崩。


## 单服务器高性能
### Process Per Connection
1. 创建进程代价高——采用进程池的方法解决
2. 父子进程通信复杂
3. 支持并发连接数量有限，进程数越多，调度代价越高。并发量为几百

### Thread Per Connection
1. 创建线程仍然拥有代价，高并发时仍存在性能问题——线程池。
2. 线程间的互斥和共享引入了复杂度，死锁
3. 某个线程异常，可能导致整个进程退出，如内存越界。

### Reactor
非阻塞同步网络模型

I/O多路复用

进程/线程池+Reactor

1. Reactor负责监听和分发事件
2. 资源池负责处理事件

Java一般使用线程如Netty，C语言使用进程或线程都可，如Nginx使用进程，Memcache使用线程。

1. 单Reactor单线程/进程
2. 单Reactor多线程
3. 多Reactor多进程/线程

Nginx采用多Reactor多进程，Memcache和Netty采用多Reactor。

### Proactor
异步网络模型，通过向内核注册I/O事件，并通过回调返回。

Windows通过IOCP实现了真正的异步I/O，Linux一般是以Reactor模式为主。

Boost.Asio在Linux下使用Reactor模式模拟出来的异步模型。

## 高性能负载均衡
DNS负载均衡实现地理级别负载均衡，硬件负载均衡实现集群级别负载均衡，软件负载均衡实现机器级别负载均衡。

### DNS负载均衡
一般用来实现地理级别的均衡，本质时DNS解析同一个域名可以返回不同的IP地址。

优点：
1. 简单、成本低，负载均衡工作由DNS服务器处理。
2. 就近访问，提升访问速度。

缺点：
1. 更新不及时，DNS缓存的时间比较长，缓存失效时，导致用户访问失败，达不到负载均衡的目的。
2. 扩展性差，DNS负载均衡的控制权在域名商，无法根据业务特点做更多的定制化功能和扩展。
3. 分配策略简单，不能根据服务器的差异来判断负载，也无法感知后端服务器的状态。

### 硬件负载均衡
优点：
1. 功能强大：全面支持各层级的负载均衡，支持全面的负载均衡算法，支持全局负载均衡。
2. 性能强大：对比一下，软件负载均衡支持到10万级并发已经很厉害了，硬件负载均衡可以支持100万以上的并发。
3. 稳定性高：商用硬件负载均衡，经过了良好的严格测试，经过大规模使用，稳定性高。
4. 支持安全防护：硬件均衡设备除具备负载均衡功能外，还具备防火墙、防DDoS攻击等安全功能。

缺点：
1. 贵
2. 扩展能力差

### 软件负载均衡
1. Nginx软件七层负载
支持HTTP、E-mail协议

2. LVS时Linux内核4层负载
与协议无关，几乎所有应用都可以做。

优点：
简单、便宜、灵活

缺点：
1. 性能一般：Nginx大约支持5万并发
2. 功能没硬件强大
3. 不具备安全功能

### 算法
#### 任务平分类
数量平均，或权重比例平均
1. 轮询
按照请求顺序轮流分配到服务器上，不关注服务器状态，当某个服务器出现bug，还是依然将请求发送给它。出现宕机，是可以感知的，将其删除。

2. 加权轮询
预先设置好根据机器性能分配好权重。

#### 负载均衡类
根据服务器的实际负载(CPU、IO、吞吐量)等

- LVS可以以连接数来判断服务器的状态，连接数越大，服务器压力越大。

- Nginx，以HTTP请求数来判断服务器状态。(本身不支持，但可以扩展)

- 根据实际 业务特点来选择指标衡量系统压力。

缺点：
1. 仅适用于负载均衡接收的任何连接请求都会转发给服务器的场景，若负载均衡与服务器之间是固定的连接池方式，则不适合。
2. 要求负载均衡系统以某种方式手机每个服务器的CPU或其他负载，复杂度上升。

#### 性能最优类
在客户端角度考虑，根据服务器响应时间来分配任务。
1. 需要收集和分析每个服务器每个任务的响应时间，大量任务场景下，本身会占用较多资源。
2. 通过采样的方式，降低性能消耗，但复杂度进一步上升，如何确定采样频率。
3. 需要选择合适的周期，是10s内性能最优还是1min中，系统上线后需要不断调优。

#### Hash类
根据任务中的某些关键字做Hash运算，满足特定的业务需求。
1. 源地址Hash
将同一个源ip地址的任务分配给同一个服务器，适合于存在事务、会话的业务。

2. ID Hash
Session id，如购物车。

# CAP
三者只能选其二。
## Consistency
一致性，读操作能保证返回的是最新的写操作结果。

## Availability
可用性，非故障节点在合理的时间内返回合理的响应。

## Partition tolerance
分区容忍性，出现网络分区后，系统能够继续履行职责。

分区是指分布式环境下，多个服务器之间出现网络故障或消息丢失。

## CP/AP
在分布式环境中，由于网络无法做到100%可靠，可能产生故障，所以分区是一个必然的现象。如果选择了CA，当发生分区现象时，为了保证C，系统需要禁止写入，当有写请求时，系统会返回error，与A冲突。


### CP
为了保证一致性，当发生分区现象后，N1的数据已经更新为y，而数据y无法同步到N2，此时客户端访问N2时，N2需要返回Error，违背了可用性。
![image-20200709174134111](C:\Users\May\AppData\Roaming\Typora\typora-user-images\image-20200709174134111.png)

### AP
为了保证可用性，当发生分区现象后，N1节点的数据已经更新为y，数据y无法同步到N2，客户端访问N2，N2返回x，违背了一致性要求。

![image-20200709174302293](C:\Users\May\AppData\Roaming\Typora\typora-user-images\image-20200709174302293.png)

## CAP细节
CAP时忽略拷贝延时的。

CAP关注的是数据而非整个系统，因此同一个系统中可以根据不同的数据实现CP/AP的抉择。

CAP理论告诉我们分布式系统只能选择CP或者CA，但其前提时发生了分区现象，因此当P不存在时，可以同时满足CA。

分区故障发生时，可以在分区期间记录一些日志，当故障解决后，可以使系统重新恢复CA状态。

以用户管理系统为例，对于账号数据，假设选择了CP，分区发生后，节点1可以继续注册新用户，节点2无法注册，此时节点1可以将新注册的信息记录到日志中，分区恢复后，将日志信息同步给节点2，重新满足CA。对于用户信息数据，如果选择AP，分区发生后，节点1和节点2都可以修改用户信息，但可能在两个节点更改不一样，分区恢复后可以按规则选择一挑，也可以返回给用户，由用户决定。

## BASE
1. Basically Available
分布式系统出现故障时，允许损失部分可用性，保证核心可用。

2. Soft State
允许系统存在中间状态，且中间状态不影响系统整体可用性，即数据不一致。

3. Eventual Consistency
所有的数据副本，经过一定事件后，最终能达到一致的状态。

# 高可用架构模式
本质时通过将数据复制到多个存储设备，通过数据冗余的方式来实现高可用，其复杂性体现在如何应对复制延迟和中断导致的数据不一致。

## 存储高可用架构
常用的高可用存储架构有主备、主从、主主、集群、分区。
### 双机
#### 主备复制
优点：
1. 对客户端不必感知备机存在，即使灾难恢复后，备机切换为主机，对于客户端来说，也只是认为主机地址更换。
2. 双方只需要进行数据复制即可，无须进行状态判断和主备切换。

缺点：
1. 备机只作为备份，没有提供读写操作，硬件成本浪费。
2. 故障后需要人工干预，无法自动回复。

#### 主从复制
主机负责读写，从机只负责读。适用于写少读多的场景。

优点：
1. 主机故障时，读操作可以继续运行。
2. 从机提供读，发挥了硬件性能。

缺点：
1. 客户端需要感知主从关系，并将不同的操作发给不同的机器进行处理。
2. 如果主从复制延迟较大，业务会出现不一致问题。
3. 故障时需要人工干预。

#### 双机切换
关键设计点：
1. 主备间状态判断
2. 切换决策
3. 数据冲突解决
##### 互连式
![img](https://static001.geekbang.org/resource/image/d5/0a/d5b87f8cc1e955279878cc3fc4b0850a.png)
主备之间直接建立状态传递通道。

为了实现自动切换：
1. 为了保证切换后不影响客户端访问，主备机采用同一个ip地址，主机绑定该ip。
2. 客户端同时记录主备机的地址，能访问哪个就访问，但备机会拒绝请求。

缺点：
1. 若状态传递通道产生故障，备机会认为主机故障而将自己升为主机，产生了两个主机。

##### 中介式
![img](https://static001.geekbang.org/resource/image/99/66/99af5ce1aefc18253839152755dedb66.png)

主备机之间无需建立管理多种类型的状态传递通道，降低了主备机连接管理的复杂度。主备机只需要将状态信息发给中介，或从中介方获取对方的状态信息。

其关键代价在于如何实现中介的高可用，若中介宕机，系统就进入了双备，不可用。

开源方案，ZooKeeper和Keepalived。

##### 模拟式
![img](https://static001.geekbang.org/resource/image/20/1b/20c26a952fcafd5a48a6220a77ff5b1b.png)
主备机之间不传递任何状态数据，二室备机模拟成一个客户端，向主机发起模拟的读写操作，根据读写操作的响应情况判断主机的状态。

模拟式读写操作获取的只有相应信息，没有状态信息的多样性。

#### 主主复制
两台都是主机，不存在切换概念，客户端无须区分，发给任意主机都可以。

必须保证数据能够双向复制，而很多数据是不能双向复制的。

1. 用户注册后生成的ID，X用户在主机A注册ID为100，此时Y用户在主机B注册ID也为100。
2. 库存不能双向复制。

一般适用于临时性、可丢失、可覆盖的数据场景，如session、日志、草稿等。

### 集群
双机架构有一个隐含假设：主机能够存储所有数据。

#### 数据集中集群
一主多备

复杂度体现在
1. 主机如何将数据复制给备机
多条复制通道会增加主机复制的压力，导致多个备机之间数据不一致，需要对备机之间的数据一致性进行检查和修正。

2. 备机如何检测主机状态
多台备机都需要对主机状态进行判断，而不同备机判断结果可能是不同的，如何处理不同备机对主机状态的不同判断。

3. 主机故障后，如何决定新的主机

ZooKeeper采用ZAB算法解决上述提到的几个问题，但ZAB算法复杂度较高。

#### 数据分散集群
多个服务器组成集群，每台服务器负责存储一部分数据和备份一部分数据。

1. 均衡性
保证数据分区基本均衡。

2. 容错性
部分服务器故障时，算法将原来分配给故障服务器的数据分区分配给其他服务器。

3. 可伸缩性
扩充服务器后，算法能自动将部分数据迁移到新服务器，并保证均衡性。

数据分散集群中，必须有一个角色负责执行数据分配算法。

Hadoop的实现就独立的服务器负责数据分区的分配，叫做Namenode。而Elasticsearch集群通过选举一台服务器做数据分区的分配，叫做master node。

    数据集中集群适合数据量不大，集群机器数量不多的场景，ZooKeeper集群一般推荐5台机器左右，数据量保证单台服务器能够支撑。
    数据分散集群，适合业务数据量巨大，集群机器数量庞大的业务场景，如Hadoop集群、HBase集群。

### 数据分区
将数据按照一定规则进行分区，不同分区分布在不同的地理位置上，避免地理级别的故障造成巨大影响。

1. 数据量
2. 分区规则
洲际、国家、城市分区
3. 复制规则
    - 集中式
    - 互备式
    - 独立式
集中式：

存在一个总的备份中心，所有的分区都将数据备份到备份中心。

优缺点：
1. 设计简单、各分区之间并无直接联系，可以做到互不影响。
2. 扩展容易。
3. 成本较高

互备式：

每个分区备份另外一个分区的数据。

优缺点：
1. 设计复杂
2. 扩展麻烦
3. 成本低

独立式：

每个分区自己有独立的备份中心，但主机与备份中心地点不同。

优缺点：
1. 设计简单
2. 扩展容易
3. 成本高

## 计算高可用架构
主要设计目标是当出现部分硬件损坏时，计算任务能够继续正常运行。本质是通过冗余来规避部分故障的风险。通过增加更多服务器来达到计算高可用。

1. 哪些服务器可以执行任务
    - 和计算高性能中的集群类似，每个服务器都可以执行任务。
    - 和存储高可用中的集群类似，只有主机可以执行任务。
2. 任务如何重新执行
    - 对于已经分配的任务即使执行失败也不做任何处理，系统只需要保证新的任务能够分配到其他非故障服务器上执行即可。
    - 设计一个任务管理器，任务执行完之后，服务器需要向任务管理器反馈执行结果，任务管理器根据结果来决定是否需要重新分配任务。

常见的计算高可用架构：主备、主从和集群。

### 主备
同主备复制架构类似，但更简单，因为无需数据复制。需要人工切换。

1. 冷备架构
备机上的程序包和配置文件都准备好，但备机上的业务系统没有启动，主机故障后，人工将备机的业务系统启动，并将任务分配器的任务请求切换发送给备机。

2. 温备架构
备机上的业务系统已经启动，只是不对外提供服务，主机故障后，人工将任务分配器的任务请求切换发送到备机即可。

适合内部管理系统、后台管理系统这种使用人数不多、频率不高的业务，不适合在线业务。

### 主从
从机也要执行任务，任务分配器需要将任务进行分类，分别发送给主机和从机。

相比于主备架构，其优缺点：
1. 从机也执行任务，发挥了从机的硬件性能。
2. 将任务分类，任务分配器更复杂。

### 集群
主备/主从，通过冗余一台服务器来提升可用性，需要人工来切换主备或主从。集群希望能够自动完成切换操作。

1. 对称集群(负载均衡集群)
集群中的每个服务器角色一样，可以执行所有任务。

关键点在于：任务分配器需要选取分配策略，任务分配器需要检测服务器状态。

2. 非对称集群
多个不同的角色，执行不同的任务，常见的有Master-Slave角色。

关键点在于：任务分配策略更加复杂，角色分配策略实现复杂(可能需要使用ZAB、Raft实现Leader的选举)。

## 业务高可用——异地多活架构
1. 正常情况下，用户无论访问哪个地点的业务系统，都能得到正确的业务服务。
2. 某个地方业务异常时，用户访问其他地方正常的业务系统，能够得到正确的业务服务。

1. 系统复杂度发生质变。
2. 成本上升。

适用场景：滴滴、支付宝、微信等，业务系统中断后，对用户影响很大。新闻网站等可以只做异地备份。

根据地理位置划分，可以分为：同城异区、跨城异地、跨国异地。

同城异区可以看作是逻辑上的同一个机房，因为距离短，传输时延低，应对机房级别故障的最优架构。

跨城异地尽可能远距离，应对极端地理、电力等灾难事件。但传输延迟高，且物理网络损坏风险高(某某某地光缆挖断了)。

对于余额等强一致性要求的数据，无法做到跨城异地多活，只采用同城异区这种架构。

跨国异地，主要是为不同地区用户提供服务，或者只读类业务多活(搜索业务，对一致性延迟要求不高)。

# 可扩展架构模式

# FMEA
Failure mode and effects analysis，故障模式与影响分析
1. 给出初始的架构设计图。
2. 假设架构中某个部件发生故障。
3. 分析此故障对系统功能造成的影响。
4. 根据分析结果，判断架构是否需要进行优化。



































